I have made a GPT-2 clone, using PyTorch mainly i managed to immitate the learning capabilities of an earlier Chat GPT model, based on the 'Attention is All You Need' paper, the model is mainly based on the transformer architecture's
post-processing part, which provides the answers to the question, the NLP side of the model being to computationally expensive for a personal device.
